--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca mpi_cuda_support 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:
--------------------------------------------------------------------------
A process has executed an operation involving a call to the
"fork()" system call to create a child process.  Open MPI is currently
operating in a condition that could result in memory corruption or
other system errors; your job may hang, crash, or produce silent
data corruption.  The use of fork() (or system() or other calls that
create child processes) is strongly discouraged.

The process that invoked fork was:

  Local host:          [[50269,1],27] (PID 2778)

If you are *absolutely sure* that your application will successfully
and correctly survive a call to fork(), you may disable this warning
by setting the mpi_warn_on_fork MCA parameter to 0.
--------------------------------------------------------------------------

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.
** Something went wrong while running addr2line. **
** Falling back to a simpler backtrace scheme. **
#12  0x2b67977a0c92
#13  0x2b679779ffc0
#14  0x2b6797c1c24f
#15  0x565a83
#16  0x5679a8
#17  0x7ccc8f
#18  0x4dd8c1
#19  0x4d35d5
#20  0x4d36c7
#21  0x2b6797c08b34
#22  0x450fa6
#23  0xffffffffffffffff
** Something went wrong while running addr2line. **
** Falling back to a simpler backtrace scheme. **
#12  0x2aae78600c92
#13  0x2aae785fffc0
#14  0x2aae78a7c24f
#15  0x565a83
#16  0x5679a8
#17  0x7ccc8f
#18  0x4dd8c1
#19  0x4d35d5
#20  0x4d36c7
#21  0x2aae78a68b34
#22  0x450fa6
#23  0xffffffffffffffff

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:
** Something went wrong while running addr2line. **
** Falling back to a simpler backtrace scheme. **
#12  0x2b8d039d0c92
#13  0x2b8d039cffc0
#14  0x2b8d03e4c24f
#15  0x565a83
#16  0x5679a8
#17  0x7ccc8f
#18  0x4dd8c1
#19  0x4d35d5
#20  0x4d36c7
#21  0x2b8d03e38b34
#22  0x450fa6
#23  0xffffffffffffffff

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:
#0  0x2ACD33C12DC7
#1  0x2ACD33C11FC0
#0  0x2B01F9292DC7
#1  0x2B01F9291FC0
#2  0x2ACD3408E24F
#2  0x2B01F970E24F
#3  0x565A83 in nb_adjust_
#3  0x565A83 in nb_adjust_
#4  0x5679A8 in ewald_force_
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2B707BC66DC7
#1  0x2B707BC65FC0#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2BA63D8BADC7
#1  0x2BA63D8B9FC0
#2  0x2BA63DD3624F
#0  0x2ADEFF6C0DC7
#1  0x2ADEFF6BFFC0
#2  0x2ADEFFB3C24F
#0  0x2ACC549A3DC7
#1  0x2ACC549A2FC0
#2  0x2ACC54E1F24F

#2  0x2B707C0E224F
#3  0x565A83 in nb_adjust_
#3  0x565A83 in nb_adjust_
#3  0x565A83 in nb_adjust_
#3  0x565A83 in nb_adjust_
#4  0x5679A8 in ewald_force_
#4  0x5679A8 in ewald_force_
#4  0x5679A8 in ewald_force_
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#5  0x7CCC8F in force_
#5  0x7CCC8F in force_
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#6  0x4DD8C1 in sander_
#6  0x4DD8C1 in sander_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2B8536E30DC7
#1  0x2B8536E2FFC0
#2  0x2B85372AC24F
#3  0x565A83 in nb_adjust_
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2AC6C144BDC7
#1  0x2AC6C144AFC0
#2  0x2AC6C18C724F
#3  0x565A83 in nb_adjust_
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#0  0x2B8E9FB6EDC7
#1  0x2B8E9FB6DFC0
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#2  0x2B8E9FFEA24F
#0  0x2B109D93BDC7
#1  0x2B109D93AFC0
#0  0x2B85BDF2EDC7
#1  0x2B85BDF2DFC0
#3  0x565A83 in nb_adjust_
#2  0x2B109DDB724F
#2  0x2B85BE3AA24F
#4  0x5679A8 in ewald_force_
#3  0x565A83 in nb_adjust_
#3  0x565A83 in nb_adjust_
#5  0x7CCC8F in force_
#4  0x5679A8 in ewald_force_
#4  0x5679A8 in ewald_force_
#6  0x4DD8C1 in sander_
#5  0x7CCC8F in force_
#5  0x7CCC8F in force_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#6  0x4DD8C1 in sander_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2AAFF27CEDC7
#1  0x2AAFF27CDFC0
#2  0x2AAFF2C4A24F
#3  0x565A83 in nb_adjust_
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2B79DA790DC7
#1  0x2B79DA78FFC0
#2  0x2B79DAC0C24F
#3  0x565A83 in nb_adjust_
#0  0x2AFA99DB2DC7
#1  0x2AFA99DB1FC0
#2  0x2AFA9A22E24F
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#3  0x565A83 in nb_adjust_
#6  0x4DD8C1 in sander_
#4  0x5679A8 in ewald_force_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
#0  0x2B74DC0BCDC7
#1  0x2B74DC0BBFC0
#2  0x2B74DC53824F
#3  0x565A83 in nb_adjust_
#4  0x5679A8 in ewald_force_
#5  0x7CCC8F in force_
#6  0x4DD8C1 in sander_
#7  0x4D35D5 in MAIN__ at multisander.F90:?
--------------------------------------------------------------------------
mpirun noticed that process rank 6 with PID 2715 on node highmem12 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
[highmem12.bc4.acrc.priv:02704] 27 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[highmem12.bc4.acrc.priv:02704] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
[highmem12.bc4.acrc.priv:02704] 18 more processes have sent help message help-opal-runtime.txt / opal_init:warn-fork
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca mpi_cuda_support 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[highmem12.bc4.acrc.priv:02819] 27 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[highmem12.bc4.acrc.priv:02819] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
--------------------------------------------------------------------------
The library attempted to open the following supporting CUDA libraries,
but each of them failed.  CUDA-aware support is disabled.
libcuda.so.1: cannot open shared object file: No such file or directory
libcuda.dylib: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.so.1: cannot open shared object file: No such file or directory
/usr/lib64/libcuda.dylib: cannot open shared object file: No such file or directory
If you are not interested in CUDA-aware support, then run with
--mca mpi_cuda_support 0 to suppress this message.  If you are interested
in CUDA-aware support, then try setting LD_LIBRARY_PATH to the location
of libcuda.so.1 to get passed this issue.
--------------------------------------------------------------------------

  Error opening unit   30: File "closed_1_min7.rst7" is missing or unreadable                
--------------------------------------------------------------------------
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
with errorcode 1.

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
[highmem12.bc4.acrc.priv:02908] 27 more processes have sent help message help-mpi-common-cuda.txt / dlopen failed
[highmem12.bc4.acrc.priv:02908] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
